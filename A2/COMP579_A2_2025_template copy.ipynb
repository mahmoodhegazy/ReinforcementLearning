{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALnFQQHn1QME"
      },
      "source": [
        "# COMP579 Assignment 2\n",
        "\n",
        "**Coding: Tabular RL [70 points]**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VlHR8XcY483f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This module implements and compares SARSA and Expected SARSA algorithms on the FrozenLake environment.\n",
        "It includes implementations of both algorithms, experiment running functionality, and visualization tools\n",
        "to analyze the performance under different hyperparameter settings.\n",
        "\n",
        "The FrozenLake environment presents a challenging reinforcement learning problem due to its\n",
        "stochastic nature (slippery ice) and sparse rewards (only received upon reaching the goal).\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gym.wrappers import StepAPICompatibility\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x, temp):\n",
        "    exps = np.exp((x - np.max(x)) / temp)\n",
        "    return exps / np.sum(exps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i4KZyIS-5LxK"
      },
      "outputs": [],
      "source": [
        "def softmax(x, temp):\n",
        "    x_adj = x - np.max(x)\n",
        "    exps = np.exp(x_adj / temp)\n",
        "    return exps / np.sum(exps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SARSA:\n",
        "    \"\"\"\n",
        "    Implementation of the SARSA (State-Action-Reward-State-Action) algorithm.\n",
        "    \n",
        "    SARSA is an on-policy temporal difference learning algorithm that learns\n",
        "    Q-values for state-action pairs through direct experience.\n",
        "    \n",
        "    Attributes:\n",
        "        env: OpenAI Gym environment\n",
        "        alpha (float): Learning rate\n",
        "        gamma (float): Discount factor for future rewards\n",
        "        temp (float): Temperature parameter for softmax exploration\n",
        "        Q (np.array): Q-value table for state-action pairs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, env, alpha, gamma, temp):\n",
        "        \"\"\"\n",
        "        Initialize SARSA agent with given parameters.\n",
        "        \n",
        "        Args:\n",
        "            env: OpenAI Gym environment\n",
        "            alpha (float): Learning rate for Q-value updates\n",
        "            gamma (float): Discount factor for future rewards\n",
        "            temp (float): Temperature parameter for softmax exploration\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.temp = temp\n",
        "        # Initialize Q-table with zeros for all state-action pairs\n",
        "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    def select_action(self, s, greedy=False):\n",
        "        \"\"\"\n",
        "        Select an action using either greedy or softmax policy.\n",
        "        \n",
        "        Args:\n",
        "            s (int): Current state\n",
        "            greedy (bool): If True, select best action; if False, use softmax exploration\n",
        "        \n",
        "        Returns:\n",
        "            int: Selected action\n",
        "        \"\"\"\n",
        "        if greedy:\n",
        "            # During evaluation, always choose the best action\n",
        "            return np.argmax(self.Q[s])\n",
        "        else:\n",
        "            # During training, use softmax exploration\n",
        "            action_probs = softmax(self.Q[s], self.temp)\n",
        "            return np.random.choice(np.arange(self.env.action_space.n), p=action_probs)\n",
        "\n",
        "    def update(self, s, a, r, s_prime, a_prime, done):\n",
        "        \"\"\"\n",
        "        Update Q-values using the SARSA update rule.\n",
        "        \n",
        "        Q(s,a) = Q(s,a) + α[r + γQ(s',a') - Q(s,a)]\n",
        "        \n",
        "        Args:\n",
        "            s (int): Current state\n",
        "            a (int): Current action\n",
        "            r (float): Reward received\n",
        "            s_prime (int): Next state\n",
        "            a_prime (int): Next action\n",
        "            done (bool): Whether episode has terminated\n",
        "        \"\"\"\n",
        "        # Calculate TD target (next state value is zero if episode is done)\n",
        "        target = r + self.gamma * self.Q[s_prime, a_prime] * (not done)\n",
        "        # Calculate TD error\n",
        "        td_error = target - self.Q[s, a]\n",
        "        # Update Q-value\n",
        "        self.Q[s, a] += self.alpha * td_error\n",
        "\n",
        "class ExpectedSARSA:\n",
        "    \"\"\"\n",
        "    Implementation of the Expected SARSA algorithm.\n",
        "    \n",
        "    Expected SARSA is a variant of SARSA that uses the expected value over next\n",
        "    actions instead of a single sampled action, potentially reducing variance\n",
        "    in the updates.\n",
        "    \n",
        "    Attributes:\n",
        "        env: OpenAI Gym environment\n",
        "        alpha (float): Learning rate\n",
        "        gamma (float): Discount factor for future rewards\n",
        "        temp (float): Temperature parameter for softmax exploration\n",
        "        Q (np.array): Q-value table for state-action pairs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, env, alpha, gamma, temp):\n",
        "        \"\"\"Initialize Expected SARSA agent with given parameters.\"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.temp = temp\n",
        "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    def select_action(self, s, greedy=False):\n",
        "        \"\"\"Select action using either greedy or softmax policy.\"\"\"\n",
        "        if greedy:\n",
        "            return np.argmax(self.Q[s])\n",
        "        else:\n",
        "            action_probs = softmax(self.Q[s], self.temp)\n",
        "            return np.random.choice(np.arange(self.env.action_space.n), p=action_probs)\n",
        "\n",
        "    def update(self, s, a, r, s_prime, a_prime, done):\n",
        "        \"\"\"\n",
        "        Update Q-values using the Expected SARSA update rule.\n",
        "        \n",
        "        Q(s,a) = Q(s,a) + α[r + γΣπ(a'|s')Q(s',a') - Q(s,a)]\n",
        "        \n",
        "        The key difference from SARSA is using the expected value over all next actions\n",
        "        weighted by their policy probabilities, rather than just the value of the\n",
        "        selected next action.\n",
        "        \"\"\"\n",
        "        # Calculate action probabilities for next state\n",
        "        action_probs = softmax(self.Q[s_prime], self.temp)\n",
        "        # Calculate expected value over all actions\n",
        "        expected_q = np.sum(action_probs * self.Q[s_prime])\n",
        "        # Calculate TD target using expected value\n",
        "        target = r + self.gamma * expected_q * (not done)\n",
        "        td_error = target - self.Q[s, a]\n",
        "        self.Q[s, a] += self.alpha * td_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sazk2zxT8jD7"
      },
      "source": [
        "# Write your experiment code below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OevfjgjL8mFk",
        "outputId": "524ca6a3-4852-4a21-9c0d-237f0a16e94f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space: Discrete(4)\n",
            "State space: Discrete(16)\n"
          ]
        }
      ],
      "source": [
        "env_name = 'FrozenLake-v1'\n",
        "env = gym.make(env_name)\n",
        "env = StepAPICompatibility(env)\n",
        "print(\"Action space:\", env.action_space)\n",
        "print(\"State space:\", env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sarsa performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "alpha_values = [0.1,0.2,0.5]\n",
        "temp_values = [0.005,0.01,0.02]\n",
        "trials = 10\n",
        "segments = 500\n",
        "episodes_per_segment = 10  # Training episodes\n",
        "testing_episode = 1  # Testing episodes per segment\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # function that runs each episode\n",
        "# def run_experiment(agent_class, env, alpha, temp, segments=500, episodes_per_segment=10, trials=10):\n",
        "#     training_results = np.zeros((trials, segments))\n",
        "#     testing_results = np.zeros((trials, segments))\n",
        "    \n",
        "#     for trial in tqdm(range(trials)):\n",
        "#         agent = agent_class(env, alpha, gamma=0.99, temp=temp)\n",
        "        \n",
        "#         for segment in range(segments):\n",
        "#             segment_rewards = []\n",
        "            \n",
        "#             # Training episodes\n",
        "#             for _ in range(episodes_per_segment):\n",
        "#                 state, _ = env.reset()\n",
        "#                 done = False\n",
        "#                 total_reward = 0\n",
        "                \n",
        "#                 while not done:\n",
        "#                     action = agent.select_action(state)\n",
        "#                     next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "#                     done = terminated or truncated\n",
        "                    \n",
        "#                     next_action = agent.select_action(next_state)\n",
        "#                     agent.update(state, action, reward, next_state, next_action, done)\n",
        "#                     action = next_action\n",
        "                    \n",
        "#                     state = next_state\n",
        "#                     total_reward += reward\n",
        "                \n",
        "#                 segment_rewards.append(total_reward)\n",
        "            \n",
        "#             training_results[trial, segment] = np.mean(segment_rewards)\n",
        "            \n",
        "#             # Testing episode\n",
        "#             state, _ = env.reset()\n",
        "#             done = False\n",
        "#             total_reward = 0\n",
        "            \n",
        "#             while not done:\n",
        "#                 action = agent.select_action(state, greedy=True)\n",
        "#                 next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "#                 done = terminated or truncated\n",
        "#                 state = next_state\n",
        "#                 total_reward += reward\n",
        "            \n",
        "#             testing_results[trial, segment] = total_reward\n",
        "    \n",
        "#     return training_results, testing_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-v-lS7pZ29Nv"
      },
      "outputs": [],
      "source": [
        "# function that runs each episode\n",
        "def run_experiment(agent_class, env, alpha, temp, segments=500, episodes_per_segment=10, trials=10):\n",
        "    \"\"\"\n",
        "    Run complete experiment for given agent and parameters.\n",
        "    \n",
        "    This function runs multiple trials of the learning process, each consisting of\n",
        "    multiple segments, where each segment contains training episodes followed by\n",
        "    a testing episode.\n",
        "    \n",
        "    Args:\n",
        "        agent_class: Class of the RL agent (SARSA or Expected SARSA)\n",
        "        env: OpenAI Gym environment\n",
        "        alpha (float): Learning rate\n",
        "        temp (float): Temperature parameter\n",
        "        segments (int): Number of segments to run\n",
        "        episodes_per_segment (int): Number of training episodes per segment\n",
        "        trials (int): Number of independent trials to run\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (training_results, testing_results) containing performance data\n",
        "    \"\"\"\n",
        "    training_results = np.zeros((trials, segments))\n",
        "    testing_results = np.zeros((trials, segments))\n",
        "    \n",
        "    for trial in tqdm(range(trials)):\n",
        "        # Initialize new agent for each trial\n",
        "        agent = agent_class(env, alpha, gamma=0.99, temp=temp)\n",
        "        \n",
        "        for segment in range(segments):\n",
        "            segment_rewards = []\n",
        "            \n",
        "            # Training episodes\n",
        "            for _ in range(episodes_per_segment):\n",
        "                state, _ = env.reset()\n",
        "                done = False\n",
        "                total_reward = 0\n",
        "                \n",
        "                while not done:\n",
        "                    # Select and execute action\n",
        "                    action = agent.select_action(state)\n",
        "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                    done = terminated or truncated\n",
        "                    \n",
        "                    # Select next action and update agent\n",
        "                    next_action = agent.select_action(next_state)\n",
        "                    agent.update(state, action, reward, next_state, next_action, done)\n",
        "                    \n",
        "                    state = next_state\n",
        "                    action = next_action\n",
        "                    total_reward += reward\n",
        "                \n",
        "                segment_rewards.append(total_reward)\n",
        "            \n",
        "            # Store average training performance for this segment\n",
        "            training_results[trial, segment] = np.mean(segment_rewards)\n",
        "            \n",
        "            # Testing episode (greedy policy)\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            total_reward = 0\n",
        "            \n",
        "            while not done:\n",
        "                action = agent.select_action(state, greedy=True)\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "            \n",
        "            testing_results[trial, segment] = total_reward\n",
        "    \n",
        "    return training_results, testing_results\n",
        "\n",
        "\n",
        "# Storage for results\n",
        "sarsa_results = {\n",
        "    'training': np.zeros((len(alpha_values), len(temp_values), 10, 500)),\n",
        "    'testing': np.zeros((len(alpha_values), len(temp_values), 10, 500))\n",
        "}\n",
        "\n",
        "expected_sarsa_results = {\n",
        "    'training': np.zeros((len(alpha_values), len(temp_values), 10, 500)),\n",
        "    'testing': np.zeros((len(alpha_values), len(temp_values), 10, 500))\n",
        "}\n",
        "\n",
        "# Run experiments for both algorithms\n",
        "for i, alpha in enumerate(alpha_values):\n",
        "    for j, temp in enumerate(temp_values):\n",
        "        print(f\"\\nRunning SARSA with alpha={alpha}, temp={temp}\")\n",
        "        train_results, test_results = run_experiment(SARSA, env, alpha, temp)\n",
        "        sarsa_results['training'][i, j] = train_results\n",
        "        sarsa_results['testing'][i, j] = test_results\n",
        "        \n",
        "        print(f\"\\nRunning Expected SARSA with alpha={alpha}, temp={temp}\")\n",
        "        train_results, test_results = run_experiment(ExpectedSARSA, env, alpha, temp)\n",
        "        expected_sarsa_results['training'][i, j] = train_results\n",
        "        expected_sarsa_results['testing'][i, j] = test_results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization code\n",
        "def plot_learning_curves(results, alpha_values, temp_values, title):\n",
        "    \"\"\"\n",
        "    Create detailed learning curves with error bands for all parameter combinations.\n",
        "    \n",
        "    Args:\n",
        "        results (dict): Dictionary containing training and testing results\n",
        "        alpha_values (list): List of learning rates used\n",
        "        temp_values (list): List of temperature values used\n",
        "        title (str): Title for the plot\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Create distinct visual styles for different parameters\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "    line_styles = ['-', '--', ':']\n",
        "    \n",
        "    for i, alpha in enumerate(alpha_values):\n",
        "        for j, temp in enumerate(temp_values):\n",
        "            # Calculate statistics across trials\n",
        "            mean_returns = np.mean(results['training'][i, j], axis=0)\n",
        "            std_error = np.std(results['training'][i, j], axis=0) / np.sqrt(10)\n",
        "            \n",
        "            # Scale x-axis by episodes per segment\n",
        "            episodes = np.arange(len(mean_returns)) * 10\n",
        "            \n",
        "            # Plot mean line with confidence bands\n",
        "            plt.plot(episodes, mean_returns, \n",
        "                    label=f'α={alpha}, T={temp}',\n",
        "                    color=colors[i],\n",
        "                    linestyle=line_styles[j],\n",
        "                    linewidth=2)\n",
        "            \n",
        "            plt.fill_between(episodes, \n",
        "                            mean_returns - std_error,\n",
        "                            mean_returns + std_error,\n",
        "                            color=colors[i],\n",
        "                            alpha=0.1)\n",
        "    \n",
        "    plt.title(f'{title}\\n(Averaged over 10 trials, 10 episodes per segment)')\n",
        "    plt.xlabel('Training Episodes')\n",
        "    plt.ylabel('Average Return per Segment')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot learning curves for both algorithms\n",
        "plot_learning_curves(sarsa_results, alpha_values, temp_values, 'SARSA Learning Curves')\n",
        "plot_learning_curves(expected_sarsa_results, alpha_values, temp_values, 'Expected SARSA Learning Curves')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
